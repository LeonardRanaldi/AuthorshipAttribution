{"cells":[{"cell_type":"markdown","metadata":{"id":"T5B7CLjz6m4J"},"source":["# Importiamo le librerie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ODhnmpf6m4R","outputId":"fa25822e-a703-4ca3-a3c0-86d5078052ca"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n"]}],"source":["import transformers\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModel,BertTokenizer, BertModel,GPT2Tokenizer, GPT2Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4PWXc0U6m4U"},"outputs":[],"source":["from torch import optim\n","from torch import nn\n","import torch.nn.functional as F\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgpdC-JL6m4V"},"outputs":[],"source":["import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm, trange\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JL5R9_or6m4V"},"outputs":[],"source":["import seaborn as sn\n","from sklearn.metrics import accuracy_score, f1_score\n","import math\n","from scipy.stats import wilcoxon"]},{"cell_type":"markdown","metadata":{"id":"ZDPy8V3f6m4W"},"source":["# Importiamo il dataset e gli alberi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_KoesYZ6m4X","outputId":"112f17a5-1e84-4b9a-f564-b048707eed2c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>author</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id26305</td>\n","      <td>This process, however, afforded me no means of...</td>\n","      <td>EAP</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id17569</td>\n","      <td>It never once occurred to me that the fumbling...</td>\n","      <td>HPL</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id11008</td>\n","      <td>In his left hand was a gold snuff box, from wh...</td>\n","      <td>EAP</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id27763</td>\n","      <td>How lovely is spring As we looked from Windsor...</td>\n","      <td>MWS</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id12958</td>\n","      <td>Finding nothing else, not even gold, the Super...</td>\n","      <td>HPL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id                                               text author\n","0  id26305  This process, however, afforded me no means of...    EAP\n","1  id17569  It never once occurred to me that the fumbling...    HPL\n","2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n","3  id27763  How lovely is spring As we looked from Windsor...    MWS\n","4  id12958  Finding nothing else, not even gold, the Super...    HPL"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","df = pd.read_csv('fede.csv')\n","\n","\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBsZWZyj6m4Y","outputId":"2f79ba5a-39d4-4f1e-c0e6-c2c15c6c8b53"},"outputs":[{"data":{"text/plain":["EAP    7900\n","MWS    6044\n","HPL    5635\n","Name: author, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.author.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I37LBTHA6m4Z"},"outputs":[],"source":["sentences = df.text.values\n","\n","target_clean_train = []\n","\n","for x in df.author:\n","    if x == 'EAP':\n","        target_clean_train.append(0)\n","    if x == 'MWS':\n","        target_clean_train.append(1)\n","    if x == 'HPL':\n","        target_clean_train.append(2)\n","        \n","labels = np.array(target_clean_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S26gokhL6m4a","outputId":"9bb04c00-7233-414d-8552-622139dad973"},"outputs":[{"data":{"text/plain":["19579"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yf1Mwwn96m4b","outputId":"a449c608-adf0-41bb-ac7d-889c42ec0fa7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 1 2] [7900 6044 5635]\n"]}],"source":["\n","unique, counts = np.unique(labels, return_counts = True)\n","\n","print(unique, counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sn3YtXRm6m4b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5Bc-wLiH6m4c"},"source":["**Funzioni utili per il calcolo statistico**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rFAklT86m4c"},"outputs":[],"source":["def calculate_mean(number_list):\n","    sum = 0\n","    for number in number_list:\n","        sum += number\n","\n","    return sum/len(number_list)\n","\n","def calculate_standard_deviation(number_list):\n","    mean = calculate_mean(number_list)\n","    summatory = 0\n","    for number in number_list:\n","        summatory += pow((number - mean),2)\n","\n","    summatory = summatory/len(number_list)\n","\n","    return math.sqrt(summatory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JTtINSR6m4d"},"outputs":[],"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    acc = np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"FY96IpLn6m4e"},"source":["## ** Transformers**\n","* **Bert**\n","    * Tokenizzatore -->  BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n","    * Modello --> BertModel.from_pretrained('bert-base-uncased')\n","\n","\n","* **Multilingua-Bert** \n","    * Tokenizzatore --> BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","    * Modello --> BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n","    \n","* **Electra** \n","    * Tokenizzatore --> ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n","    * Modello --> ElectraModel.from_pretrained('google/electra-base-discriminator')\n","    \n","* **XLNet** \n","    * Tokenizzatore --> XLNetTokenizer.from_pretrained('xlnet-base-cased')\n","    * Modello --> XLNetModel.from_pretrained('xlnet-base-cased')\n","    \n","* **Ernie** \n","    * AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n","    * AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\")"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HLI-rYb56m4e","executionInfo":{"status":"ok","timestamp":1645092610783,"user_tz":-60,"elapsed":788,"user":{"displayName":"federico ranaldi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02151323048761655178"}}},"outputs":[],"source":["def define_input(seed, random_state, sentences, model_type, epochs):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","        \n","    global tokenizer\n","    global model_architecture\n","        \n","    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","    \n","    if model_type == 'Bert':\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n","        model_architecture = BertModel.from_pretrained('bert-base-uncased').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'Electra':\n","        tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n","        model_architecture = AutoModel.from_pretrained(\"google/electra-base-discriminator\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","                \n","    if model_type == 'XLNet':\n","        tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n","        model_architecture = AutoModel.from_pretrained(\"xlnet-base-cased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'Multilingua-Bert':\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","        model_architecture = BertModel.from_pretrained(\"bert-base-multilingual-uncased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'Ernie':\n","        tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n","        model_architecture = AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'gpt':\n","        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","        model_architecture = AutoModel.from_pretrained(\"gpt2\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    \n","    \n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    \n","    MAX_LEN = 128\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    attention_masks = []\n","\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","        \n","    X_inputs, test_inputs, X_labels, test_labels = train_test_split(input_ids, labels, random_state=random_state, test_size=0.1)\n","    X_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=random_state, test_size=0.1)\n","    #X_trees, test_trees, _, _ = train_test_split(trees, input_ids, random_state=random_state, test_size=0.1)\n","\n","    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X_inputs, X_labels, random_state=random_state, test_size=0.1)\n","    train_masks, validation_masks, _, _ = train_test_split(X_masks, X_inputs, random_state=random_state, test_size=0.1)\n","    #train_trees, validation_trees, _, _ = train_test_split(X_trees, X_inputs, random_state=random_state, test_size=0.1)\n","\n","    train_inputs = torch.tensor(train_inputs)\n","    train_labels = torch.tensor(train_labels)\n","    train_masks = torch.tensor(train_masks)\n","    #train_trees = torch.stack(train_trees)\n","\n","    validation_inputs = torch.tensor(validation_inputs)\n","    validation_labels = torch.tensor(validation_labels)\n","    validation_masks = torch.tensor(validation_masks)\n","    #validation_trees = torch.stack(validation_trees)\n","\n","    test_inputs = torch.tensor(test_inputs)\n","    test_labels = torch.tensor(test_labels)\n","    test_masks = torch.tensor(test_masks)\n","    #test_trees = torch.stack(test_trees)\n","\n","    batch_size = 32\n","\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    \n","    return train_dataloader, validation_dataloader, test_dataloader, model_architecture, device, test_labels\n","    "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"zHsARhBl6m4j","executionInfo":{"status":"ok","timestamp":1645092610787,"user_tz":-60,"elapsed":15,"user":{"displayName":"federico ranaldi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02151323048761655178"}}},"outputs":[],"source":["def define_input_with_test(seed, random_state, sentences , sentences_test, model_type, epochs):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","        \n","    global tokenizer\n","    global model_architecture\n","        \n","    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n","    sentences_test = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences_test]\n","\n","    \n","    if model_type == 'Bert':\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n","        model_architecture = BertModel.from_pretrained('bert-base-uncased').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'Electra':\n","        tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n","        model_architecture = AutoModel.from_pretrained(\"google/electra-base-discriminator\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","                \n","    if model_type == 'XLNet':\n","        tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n","        model_architecture = AutoModel.from_pretrained(\"xlnet-base-cased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    if model_type == 'Multilingua-Bert':\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","        model_architecture = BertModel.from_pretrained(\"bert-base-multilingual-uncased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        \n","    if model_type == 'Ernie':\n","        tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n","        model_architecture = AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    \n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    tokenized_texts_test = [tokenizer.tokenize(sent) for sent in sentences_test]\n","\n","    \n","    MAX_LEN = 128\n","    \n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    attention_masks = []\n","\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","        \n","    attention_masks_test = []\n","\n","    for seq in input_ids_test:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks_test.append(seq_mask)\n","        \n","    X_inputs, _, X_labels, _ = train_test_split(input_ids, labels, random_state=random_state, test_size=0.1)\n","    X_masks, _, _, _ = train_test_split(attention_masks, input_ids, random_state=random_state, test_size=0.1)\n","    #X_trees, test_trees, _, _ = train_test_split(trees, input_ids, random_state=random_state, test_size=0.1)\n","\n","    test_inputs,_, test_labels,_ = train_test_split(input_ids_test, labels_test, random_state=random_state, test_size=0.01)\n","    test_masks, _, _, _ = train_test_split(attention_masks_test, input_ids_test, random_state=random_state, test_size=0.01)\n","\n","    \n","    \n","    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X_inputs, X_labels, random_state=random_state, test_size=0.1)\n","    train_masks, validation_masks, _, _ = train_test_split(X_masks, X_inputs, random_state=random_state, test_size=0.1)\n","    #train_trees, validation_trees, _, _ = train_test_split(X_trees, X_inputs, random_state=random_state, test_size=0.1)\n","\n","    train_inputs = torch.tensor(train_inputs)\n","    train_labels = torch.tensor(train_labels)\n","    train_masks = torch.tensor(train_masks)\n","    #train_trees = torch.stack(train_trees)\n","\n","    validation_inputs = torch.tensor(validation_inputs)\n","    validation_labels = torch.tensor(validation_labels)\n","    validation_masks = torch.tensor(validation_masks)\n","    #validation_trees = torch.stack(validation_trees)\n","\n","    test_inputs = torch.tensor(test_inputs)\n","    test_labels = torch.tensor(test_labels)\n","    test_masks = torch.tensor(test_masks)\n","    #test_trees = torch.stack(test_trees)\n","\n","    batch_size = 32\n","\n","    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    \n","    return train_dataloader, validation_dataloader, test_dataloader, model_architecture, device, test_labels\n","    "]},{"cell_type":"markdown","metadata":{"id":"_imsrAd86m4l"},"source":["# Definiamo i modelli che utilizzeremo"]},{"cell_type":"markdown","metadata":{"id":"KqFQI7Yw6m4m"},"source":["### Solo Bert"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"fkwtuGc_6m4m","executionInfo":{"status":"error","timestamp":1645089035083,"user_tz":-60,"elapsed":325,"user":{"displayName":"federico ranaldi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02151323048761655178"}},"outputId":"b6d8ebb9-b684-4498-b6a4-5a6fd3ad17c7"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2772c05a3998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mModel_Transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["class Model_Transformer(nn.Module):\n","  \n","    def __init__(self, input_dim_bert, output_dim, model_architecture):\n","        super().__init__()\n","        self.bert = model_architecture\n","        \n","        self.dropout = nn.Dropout(0.1)\n","        self.sem_linear = nn.Linear(input_dim_bert, output_dim)\n","        \n","    def forward(self, x_sem, attention_mask):\n","        with torch.no_grad():\n","            pooled_output = self.bert(x_sem, attention_mask)[0][:, 0, :]  \n","            pooled_output = self.dropout(pooled_output)\n","        logits = self.sem_linear(pooled_output)\n","\n","        return logits\n","    \n","class Model_Transformer_2(nn.Module):\n","  \n","    def __init__(self, input_dim_bert, output_dim, model_architecture):\n","        super().__init__()\n","        self.bert = model_architecture\n","        \n","        self.dropout = nn.Dropout(0.1)\n","        self.sem_linear = nn.Linear(input_dim_bert, output_dim)\n","        \n","    def forward(self, x_sem, attention_mask):\n","        \n","        gpt_out = self.bert(x_sem)[0] #returns tuple\n","        poled_output = self.dropout(gpt_out.shape[0]) \n","        logits = self.sem_linear(gpt_out.view(batch_size,-1))\n","\n","        #logits = self.sem_linear(pooled_output)\n","\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"9e5-rrNp6m4n"},"source":["# Creiamo le funzioni di applicazione dei modelli"]},{"cell_type":"markdown","metadata":{"id":"2t5WYopz6m4n"},"source":["### Esecuzione di solo Bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6LWLcQk6m4o"},"outputs":[],"source":["def execute_Transformer(epochs, model_architecture, train_dataloader, validation_dataloader, test_dataloader, device, test_labels):\n","    \n","    Alone_model = Model_Transformer(768,3, model_architecture)\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    parameters = filter(lambda p: p.requires_grad, Alone_model.parameters())\n","    optimizer = optim.AdamW(Alone_model.parameters(), lr=2e-5)\n","\n","    Alone_model.cuda()\n","\n","    # Store our loss and accuracy for plotting\n","    train_loss_set = []\n","    # Number of training epochs \n","    epoch = 0\n","\n","    # BERT training loop\n","    for _ in trange(epochs, desc=\"Epoch\"):  \n","        Alone_model.train()  \n","          # Tracking variables\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","      # Train the data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            #print(step, batch)\n","            # Add batch to GPU\n","            batch = tuple(t.cuda() for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Clear out the gradients (by default they accumulate)\n","            optimizer.zero_grad()\n","            # Forward pass\n","\n","            target_hat = Alone_model(b_input_ids, b_input_mask)\n","\n","            loss = criterion(target_hat, b_labels)\n","            train_loss_set.append(loss.item())\n","\n","            # Backward pass\n","            loss.backward()\n","            # Update parameters and take a step using the computed gradient\n","            optimizer.step()\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","\n","        ## VALIDATION\n","\n","      # Put model in evaluation mode\n","        Alone_model.eval()\n","        # Tracking variables \n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU\n","            batch = tuple(t.cuda() for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","\n","              # Forward pass, calculate logit predictions\n","\n","              logits = Alone_model(b_input_ids, b_input_mask)\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","\n","        epoch +=1\n","\n","    predictions = []\n","    Alone_model.eval()\n","\n","    for batch in test_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        with torch.no_grad():\n","            logits = Alone_model(b_input_ids, b_input_mask)\n","        logits = logits.detach().cpu().numpy()\n","\n","        predictions.append(logits)\n","\n","        flat_predictions = [item for sublist in predictions for item in sublist]\n","        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","        \n","    A = accuracy_score(test_labels.numpy(), flat_predictions)\n","    B = f1_score(test_labels.numpy(), flat_predictions, average='macro')\n","    C = f1_score(test_labels.numpy(), flat_predictions, average='weighted')\n","    D = f1_score(test_labels.numpy(), flat_predictions, average=None)\n","        \n","    return A,B,C,D"]},{"cell_type":"markdown","metadata":{"id":"LSIWFv_s6m4p"},"source":["# Eseguiamo le funzioni"]},{"cell_type":"markdown","metadata":{"id":"Eu8O7o456m4p"},"source":["**Definiamo le varibili che univoche per tutti i modelli**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTZtc9uA6m4p"},"outputs":[],"source":["seed = [46, 23, 17, 54, 31]\n","random_state = [1024, 3333, 1995, 2780, 3833]\n","model_architecture_list = ['Bert','XLNet','Multilingua-Bert', 'Electra', 'Ernie','gpt']\n","epochs = 5\n","\n","model_architecture = model_architecture_list[0]"]},{"cell_type":"markdown","metadata":{"id":"PMrH5TV06m4p"},"source":["**Eseguiamo solo un Transformer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh0ejnzQ6m4q","outputId":"18356865-554c-41c3-80ca-665852fbae73"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [18:14<00:00, 218.95s/it]\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"]}],"source":["Bert_accuracy_list = []\n","Bert_macro_list = []\n","Bert_weighted_list = []\n","Bert_other_0 = []\n","Bert_other_1 = []\n","\n","\n","for i in range(0, 5):\n","    #train_dataloder, validation_dataloader, test_dataloder, model_architecture, device, test_labels = define_input_with_test(seed[i], random_state[i], sentences, sentences_test, model_architecture, epochs)\n","    \n","    train_dataloder, validation_dataloader, test_dataloder, model_architecture, device, test_labels = define_input(seed[i], random_state[i], sentences, model_architecture, epochs)\n","\n","    \n","    A,B,C,D = execute_Transformer(epochs, model_architecture, train_dataloder, validation_dataloader, test_dataloder, device, test_labels)\n","    \n","    Bert_accuracy_list.append(A*100)\n","    Bert_macro_list.append(B)\n","    Bert_weighted_list.append(C)\n","    Bert_other_0.append(D[0])\n","    Bert_other_1.append(D[1])"]},{"cell_type":"markdown","metadata":{"id":"z7qOYcjx6m4q"},"source":["# Calcoliamo la media e la variazione standard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTllSHHX6m4q"},"outputs":[],"source":["print('*********** F1 - SCORE CLASS 0')\n","print(round(calculate_mean(Bert_other_0), 2))\n","print(round(calculate_standard_deviation(Bert_other_0), 2))\n","\n","print('*********** F1 - SCORE CLASS 1')\n","print(round(calculate_mean(Bert_other_1), 2))\n","print(round(calculate_standard_deviation(Bert_other_1), 2))\n","\n","print('*********** ACCURACY')\n","print(round(calculate_mean(Bert_accuracy_list),2))\n","print(round(calculate_standard_deviation(Bert_accuracy_list),2))\n","\n","print('*********** MACRO')\n","print(round(calculate_mean(Bert_macro_list),2))\n","print(round(calculate_standard_deviation(Bert_macro_list),2))\n","\n","print('*********** WEIGHTED')\n","print(round(calculate_mean(Bert_weighted_list), 2))\n","print(round(calculate_standard_deviation(Bert_weighted_list), 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAzK1nhv6m4r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Me2XUa_6m4r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LE3ztsj56m4r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9VQGTDPr6m4r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BlIBSpU6m4r"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztLTCgBl6m4s"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7OMbdYW6m4s"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"06f3IViJ6m4s"},"source":["# Verifichiamo se i risultati ottenuti sono statisticamente significativi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuTXYjjP6m4s"},"outputs":[],"source":["def p_test(p):\n","    alpha = 0.05\n","    if p > alpha:\n","        print('Same distribution (fail to reject H0)')\n","    else:\n","        print('Different distribution (reject H0)')"]},{"cell_type":"markdown","metadata":{"id":"JexMM91g6m4s"},"source":["**Potter vs Kermit**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5q9-y9K6m4s","outputId":"de9b5720-a149-43b7-b611-12c57a9f0fd3"},"outputs":[{"ename":"NameError","evalue":"name 'Potter_accuracy_list' is not defined","output_type":"error","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-21-42d9498ca874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwilcoxon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPotter_accuracy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKermit_accuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mm_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwilcoxon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPotter_macro_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKermit_macro_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwilcoxon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPotter_weighted_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKermit_weighted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Potter_accuracy_list' is not defined"]}],"source":["a_w, a_p = wilcoxon(Potter_accuracy_list, Kermit_accuracy_list)\n","m_w, m_p = wilcoxon(Potter_macro_list, Kermit_macro_list)\n","w_w, w_p = wilcoxon(Potter_weighted_list, Kermit_weighted_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLi2dAtg6m4t"},"outputs":[],"source":["p_test(a_p)\n","p_test(m_p)\n","p_test(w_p)"]},{"cell_type":"markdown","metadata":{"id":"-BMT9Oy06m4t"},"source":["**Potter vs Bert**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVbRgsQB6m4t"},"outputs":[],"source":["a_w, a_p = wilcoxon(Potter_accuracy_list, Bert_accuracy_list)\n","m_w, m_p = wilcoxon(Potter_macro_list, Bert_macro_list)\n","w_w, w_p = wilcoxon(Potter_weighted_list, Bert_weighted_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0T2aXWdN6m4u"},"outputs":[],"source":["p_test(a_p)\n","p_test(m_p)\n","p_test(w_p)"]},{"cell_type":"markdown","metadata":{"id":"OaNDMIcD6m4u"},"source":["**Kermit vs Bert**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dE2VEiI6m4u"},"outputs":[],"source":["a_w, a_p = wilcoxon(Kermit_accuracy_list, Bert_accuracy_list)\n","m_w, m_p = wilcoxon(Kermit_macro_list, Bert_macro_list)\n","w_w, w_p = wilcoxon(Kermit_weighted_list, Bert_weighted_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hf4rS2zh6m4u"},"outputs":[],"source":["p_test(a_p)\n","p_test(m_p)\n","p_test(w_p)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"SWea04AA6m4u"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"Transf_Based.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}