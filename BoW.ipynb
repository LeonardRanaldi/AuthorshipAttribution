{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J-pSlp33Jq0"
   },
   "source": [
    "## **AUTHOR RECOGNITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U5aXtoF29DO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQu2J5UO3K_e"
   },
   "outputs": [],
   "source": [
    "#DATASET spooky authors\n",
    "\n",
    "#df=pd.read_csv('/content/top7_EL.csv')\n",
    "#df=pd.read_csv('/content/train.csv')\n",
    "df=pd.read_csv('/content/top11_EL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JIeXRovs3p1d",
    "outputId": "aa6ef01b-f5e0-4f53-97a2-53c0addc5216"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgcn42Rq30ES",
    "outputId": "04c4c52d-2ad2-4a73-8ca4-64799549ea7e"
   },
   "outputs": [],
   "source": [
    "df[\"author\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_7xUQqEltZx"
   },
   "outputs": [],
   "source": [
    "#df=df[df[\"author\"].isin([\"Mary Wollstonecraft Shelley\",\"Howard Phillips Lovecraft\",\"Edgar Allan Poe\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vowljOCL7eoE"
   },
   "outputs": [],
   "source": [
    "df.index=range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "oRnbvCYwYB-G",
    "outputId": "b1c996ec-7690-402a-d832-aca2599c9b84"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"author\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb0GZdf_VLTf"
   },
   "source": [
    "### **PREPROCESSING:clean and tokenize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12dLdmmn3kKv"
   },
   "source": [
    "TOKENIZZAZIONE PER PAROLE:un testo può essere concepito come lista di frasi come lista di stringhe(parole,punteggiatura,numeri...).La tokenizzazione per parole prende un testo e lo trasforma in una lista di stringhe dette token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCX3t6tHStmj",
    "outputId": "33be6815-6dd1-4f06-8ab6-d0d7077339b8"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "L9QolNDQVE3Z",
    "outputId": "d3658327-5a0d-4535-c57c-b9eb16ccc8df"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIvQY1-2agMZ",
    "outputId": "d3936cf1-5283-45bc-836c-7a93269a0201"
   },
   "outputs": [],
   "source": [
    "type(df.loc[10,\"tokenized_sents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKEPtauJ9qKu",
    "outputId": "db610efe-95ad-4232-d0bd-59aad3ee58b6"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKsx3XNq9Z0a",
    "outputId": "1d6772b1-3753-403d-f58b-71d7b978be22"
   },
   "outputs": [],
   "source": [
    "type(list(df[\"tokenized_sents\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6n7-GfRW9jp9",
    "outputId": "f3b0d6e3-ecaa-4ae5-89e6-21b27d9477d2"
   },
   "outputs": [],
   "source": [
    "print(len(list(df[\"tokenized_sents\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgTcD5Yh6j5a"
   },
   "source": [
    "RIMOZIONE DELLE STOPWORDS:un testo potrebbe contenere parole che ai fini di una determinata analisi potrebbero non servire e quindi conviene eliminarle.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc56tGB-7AkX",
    "outputId": "4cca9a92-eab8-4a5d-cf40-9dc6061916c6"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "noises_list=list(string.punctuation)+[\"-\"+\"--\"+\"\\n\"+\" \"+'\"\"'+\"\"]\n",
    "stop_list = stopwords.words('english')+list(string.punctuation) #noise removal:insieme alle stopwords si elimina la punteggiatura\n",
    "\n",
    "\n",
    "print(stop_list)\n",
    "print(noises_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5GTbjyRVale"
   },
   "outputs": [],
   "source": [
    "df['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: ' '.join([token for token in x if token not in (noises_list)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jITn0370UnIc",
    "outputId": "b92bfd1f-344f-47a3-ee89-85e93b197bfc"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIMW7Ap9THXS",
    "outputId": "ee187b29-09f8-40c8-f0b8-6f66147f7fe6"
   },
   "outputs": [],
   "source": [
    "type(df.loc[10,\"tokenized_sents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwFFdXJi7uBQ"
   },
   "source": [
    "TRASFORMAZIONE IN LOWERCASE DI TUTTE LE PAROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMSyxwVc7ckE"
   },
   "outputs": [],
   "source": [
    "df['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FOxiLssxT6l9",
    "outputId": "64a94869-8acc-49ac-f7f3-4c1a5ea33fc4"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVlGgodNASvm",
    "outputId": "cad8da95-c4e2-4f20-f6bf-3f960c580517"
   },
   "outputs": [],
   "source": [
    "type(df.loc[10,\"tokenized_sents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVwaGwFg4hiU"
   },
   "source": [
    "# **ANALISI DEL DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "m_kwrbLR4bLD",
    "outputId": "da154fdb-5588-4c71-947a-3b993dfa369c"
   },
   "outputs": [],
   "source": [
    "authors=df[\"author\"].unique()\n",
    "a=np.zeros(len(authors))\n",
    "d = {'Authors': authors, 'NumberOfTexts': a}\n",
    "stats = pd.DataFrame(data=d)\n",
    "for i in range(len(authors)):\n",
    "  stats.loc[stats[\"Authors\"]==authors[i],\"NumberOfTexts\"]+=len(df.loc[df[\"author\"]==authors[i]])\n",
    "\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9-6Zdrt6E8W"
   },
   "outputs": [],
   "source": [
    "for i in range(len(stats)):\n",
    "  stats.loc[i,\"NumberOfTexts\"]=(stats.loc[i,\"NumberOfTexts\"]/len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGI9cWEP9O_D"
   },
   "source": [
    "Oss:il dataset è abbastanza bilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "8_Y8V8py8mJI",
    "outputId": "701ef922-0f35-407b-e98c-828cdcd421e2"
   },
   "outputs": [],
   "source": [
    "print(\"Autori con rispettiva percentuale delle opere nel dataset:\")\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypNTqk-4-E8G"
   },
   "source": [
    "# **Vettorizzazione con BOW**\n",
    "L'algoritmo di vettorizzazione Bag-Of-Words si occupa di trasformare un testo suddiviso in token in un vettore di dati numerici.\n",
    "L'algoritmo va a traformare un dato in un vettore di caratteristiche(features).\n",
    "Nell'algoritmo BOW le features sono rappresentate dai token.\n",
    "Viene creato un vocabolario che assegna ad ogni token che in questo caso è una feature un intero.Questo intero rappresenta l'indice della feature all'interno del vettore rappresentante il dato.\n",
    "Ogni dato verrà codificato con un vettore i cui valori sono impostati rispetto a questo dizionario.\n",
    "Quindi il vettore corrispondente ad un dato avrà in corrispondenza di ogni indice il numero di occorrenze di quella parola basandosi sul dizionario che ha creato.\n",
    "Con l'algoritmo BOW non viene codificato l'ordine dei token all'interno di un dato ma solo le occorrenze.\n",
    "Quindi c'è una perdita di informazione significativa legata all'ordine dei token all'interno della frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSaQzsTy9MCN",
    "outputId": "68558c30-9bd2-4e19-b817-ea286452e56b"
   },
   "outputs": [],
   "source": [
    "#trasformazione delle features da stringhe a numeri\n",
    "#memo:le features sono i testi\n",
    "\n",
    "#Verrà usato l'algoritmo BOW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#\n",
    "# Create sample set of documents\n",
    "#\n",
    "docs = np.array(df[\"tokenized_sents\"])\n",
    "\n",
    "#\n",
    "# Fit the bag-of-words model\n",
    "#\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "#\n",
    "# Get unique words / tokens found in all the documents. The unique words / tokens represents\n",
    "# the features\n",
    "#\n",
    "print(vectorizer.get_feature_names())\n",
    "#\n",
    "# Associate the indices with each unique word\n",
    "#\n",
    "print(vectorizer.vocabulary_)\n",
    "#\n",
    "# Print the numerical feature vector\n",
    "#\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNybfKyd-c3i",
    "outputId": "3a1eccda-0351-4282-8b11-a6ff5077e287"
   },
   "outputs": [],
   "source": [
    "X=bag.toarray()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGPy2O8cVt4s",
    "outputId": "1aecdec3-f00d-4251-ce83-5440e10c9144"
   },
   "outputs": [],
   "source": [
    "Y=np.empty((len(df),1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WFpMYVs_bcj",
    "outputId": "ffa61b9d-0db5-4974-9a51-933f609180e5"
   },
   "outputs": [],
   "source": [
    "#trasformazione delle classi targhet da stringhe a numeri\n",
    "authors=df[\"author\"].unique() #lista/insieme degli autori\n",
    "\n",
    "targets=np.array(df[\"author\"])\n",
    "\n",
    "for i in range (len(df)):\n",
    "  Y[i]=np.where(authors == targets[i])[0][0]\n",
    "\n",
    "Y=Y.astype(\"int\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yuUbIf8WDOt"
   },
   "source": [
    "Splitting dei vettori delle feautures e dei targhet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpilcn9jXYFp",
    "outputId": "a13fa4a0-a837-4086-f3fe-afa12818aeea"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMf1EeUQXc9m",
    "outputId": "832d5d0a-cf53-4d13-b8fa-a2e361b05b33"
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K9NGnsdWCNk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fp3KPQ6lYS8j",
    "outputId": "03a9d157-da01-4e86-fcfe-e5b56640f37c"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2Holn9OXI7I",
    "outputId": "910dfca2-b8fa-41ce-f989-dff8821f61c5"
   },
   "outputs": [],
   "source": [
    "print(\"Xtrain\",type(X_train))\n",
    "print(\"Ytrain\",type(Y_train))\n",
    "print(\"Xtest\",type(X_test))\n",
    "print(\"Ytest\",type(Y_test))\n",
    "print(\"Xtrain\",X_train.dtype)\n",
    "print(\"Ytrain\",Y_train.dtype)\n",
    "print(\"Xtest\",X_test.dtype)\n",
    "print(\"Ytest\",Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaG7drVrACn5"
   },
   "source": [
    "**Trasformazione degli array numpy in tensori pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxBGoLQN_j5Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_test = torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgaKUcChCZz5",
    "outputId": "df13ab8d-9136-4603-eb56-2ada7b1d8653"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8JZDR6HCGnq"
   },
   "source": [
    "# Creazione della rete neurale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2EjWHC5btux",
    "outputId": "df8d4b64-d625-416d-c47a-f7be060e2ed2"
   },
   "outputs": [],
   "source": [
    "#il numero di neuroni viene fatto corrispondere al numero di colonne di un vettore rappresentante un dato\n",
    "#cioè in base al numero di termini che hanno una tfidf maggiore di 0\n",
    "\n",
    "len(X_train[0])\n",
    "\n",
    "input_layer_neurons=len(X_train[0])\n",
    "output_layer_neurons=3\n",
    "\n",
    "print(input_layer_neurons)\n",
    "\n",
    "print(output_layer_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHFjVhe6AMiq"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "#creazione della classe Net\n",
    "#Net è una sottoclasse di nn.Module quindi eredita tutti i suoi metodi e attributi\n",
    "class Net(nn.Module):\n",
    "  def __init__(self): #costruttore della classe\n",
    "    super().__init__()  #viene richiamato il costruttore della classe nn.Module\n",
    "\n",
    "    #definizione dei layers(quindi degli strati della rete che avranno un numero di ingressi e un numero di uscite determinati)\n",
    "    #nn.linear applica una trasformazione lineare ai dati\n",
    "    self.fc1=nn.Linear(input_layer_neurons,124)  #Input Layer : ha 28*28 ingressi(quindi 784 neuroni) e 64 uscite possibili(cioé gli ingressi del layer successivo saranno 64)\n",
    "    self.fc2=nn.Linear(124,64)     #Hidden Layer 1: ha 64 ingressi e 64 uscite\n",
    "    self.fc3=nn.Linear(64,64)     #Hidden Layer 2: ha 64 ingressi e 64 uscite\n",
    "    self.fc4=nn.Linear(64,output_layer_neurons)     #Output Layer : ha 64 ingressi e 10 uscite perché le classi possibili sono 10\n",
    "  \n",
    "  #definizione della logica secondo cui i dati attraversano i vari layer\n",
    "  #ad ogni dato quando attraversa ogni neurone viene applicata la funzione relu\n",
    "  def forward(self,x):\n",
    "    x=fn.relu(self.fc1(x))\n",
    "    x=fn.relu(self.fc2(x))\n",
    "    x=fn.relu(self.fc3(x))\n",
    "    x=self.fc4(x)\n",
    "\n",
    "    #return x\n",
    "    return fn.log_softmax(x,dim=1)  #ritorna la distribuzione probabilistica dell'output della rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZFkZymiC6-j"
   },
   "outputs": [],
   "source": [
    "net=Net() #istanziazione della rete neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvRzCMHLBtWQ"
   },
   "source": [
    "# Caricamento dei dati(comprende la suddivisione in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtBK2WGgFr-R"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_data = TensorDataset(X_train,Y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test,Y_test)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJLjWvLdE00v"
   },
   "source": [
    "# Addestramento del modello(FASE DI TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK2N8sm7c1a4",
    "outputId": "8e52cfcb-bb12-4d5b-e1b8-e822a4239cb0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"gpu available\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxw7LOAfC-iF",
    "outputId": "459fa731-dbb3-4b74-b74a-8acac8021fcc"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#implementazione della loss-function o funzione di costo\n",
    "#la loss-function calcola quanto gli output della rete neurale si discostano dai valori reali\n",
    "#il risultato di una loss-function può essere uno scalare(entropia incrociata...) oppure un vettore(one-hot array)\n",
    "\n",
    "loss_function=nn.CrossEntropyLoss();\n",
    "\n",
    "#implementazione dell'ottimizzatore \n",
    "#l'ottimizzatore si occupa di modificare i pesi dei collegamenti della rete in modo che questa restituisca un output che si avvicini il più possibile al valore corretto\n",
    "#net.parameters() restituisce un tensore che rappresenta i parametri della rete ossia tutti i pesi della rete\n",
    "#il learning rate (lr) è un valore che dice di quanto devono essere aggiustati i pesi a ogni iterazione\n",
    "#Adam è un ottimizzatore che implementa la discesa stocastica del gradiente (Stochastic Gradient Descent)\n",
    "\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "#l'epoch è rappresenta l'intera fase in cui al modello sono stati passati tutti i dati del trainset suddivisi in batches\n",
    "#se si effettuano poche epoche il modello non si addestra bene o meglio non aggiusta i propri pesi nel modo migliore\n",
    "#se si effettuano troppe epoche il modello rischia l'overfitting (si adegua troppo al training set e perde la capacità di generalizzare)\n",
    "\n",
    "for epoch in range(3):\n",
    "  for data in train_data:\n",
    "    X, y = data #suddividiamo l'esempio in features (X) e classe di appartenenza (y)\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "    net.zero_grad() #setta il gradiente a 0\n",
    "    output=net(X.view(-1,input_layer_neurons))  #ad ogni passo alla rete viene fornito come input l'ouput dell'iterazione precedente\n",
    "    #print(output)\n",
    "    loss=fn.nll_loss(output,y.long())  #calcola la loss-function sull'output della rete al passo corrente rispetto al target reale\n",
    "    loss.backward() #l'errore commesso viene propagato verso ogni collegamento dallo strato di output agli strati più interni(back-propagation)\n",
    "    optimizer.step()  #basandosi sui valori riportati dal gradiente effettua l'aggiustamento dei parametri della rete in maniera opportuna  \n",
    "  print(loss) #stampa la loss function dopo ogni epoca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMxDv05FaidP"
   },
   "source": [
    "## Valutazione del modello(FASE DI VALIDAZIONE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "LYZ8dmyORg_t",
    "outputId": "d37b9ab4-1bef-437f-8064-df6d00632f2a"
   },
   "outputs": [],
   "source": [
    "correct=0 #numero di esempi su cui la rete ha predetto in maniera corretta\n",
    "total=0 #numero di esempi su cui la rete ha effettuato una predizione\n",
    "\n",
    "with torch.no_grad(): #nella fase di test non dobbiamo aggiustare i pesi o calcolare la loss function percui non ci interessa il gradiente\n",
    "  for data in test_data:\n",
    "    X, y = data\n",
    "    output=net(X.view(-1,input_layer_neurons))\n",
    "    #print(output)\n",
    "    for idx, i in enumerate(output):\n",
    "      #print(torch.argmax(i),y[idx])\n",
    "      if(torch.argmax(i)==y[idx]):\n",
    "        correct+=1\n",
    "      total+=1\n",
    "\n",
    "print(\"ACCURACY: \",round(correct/total,3)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFfTbndMVMg9"
   },
   "source": [
    "RISULTATI:\n",
    "\n",
    "3 autori: 77.1%, 75.6%, 77.5%  \"Mary Wollstonecraft Shelley\",\"Howard Phillips Lovecraft\",\"Edgar Allan Poe\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AUTHOR_REC_BOW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
